{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import joblib\n",
    "import argparse\n",
    "import warnings\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    classification_report,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_nucleosides = [\"A\", \"C\", \"G\", \"T\"]\n",
    "\n",
    "di_nucleosides = [\n",
    "    \"AA\",\n",
    "    \"AC\",\n",
    "    \"AG\",\n",
    "    \"AT\",\n",
    "    \"CA\",\n",
    "    \"CC\",\n",
    "    \"CG\",\n",
    "    \"CT\",\n",
    "    \"GA\",\n",
    "    \"GC\",\n",
    "    \"GG\",\n",
    "    \"GT\",\n",
    "    \"TA\",\n",
    "    \"TC\",\n",
    "    \"TG\",\n",
    "    \"TT\",\n",
    "]\n",
    "\n",
    "tri_nucleosides = [\n",
    "    \"AAA\",\n",
    "    \"AAC\",\n",
    "    \"AAG\",\n",
    "    \"AAT\",\n",
    "    \"ACA\",\n",
    "    \"ACC\",\n",
    "    \"ACG\",\n",
    "    \"ACT\",\n",
    "    \"AGA\",\n",
    "    \"AGC\",\n",
    "    \"AGG\",\n",
    "    \"AGT\",\n",
    "    \"ATA\",\n",
    "    \"ATC\",\n",
    "    \"ATG\",\n",
    "    \"ATT\",\n",
    "    \"CAA\",\n",
    "    \"CAC\",\n",
    "    \"CAG\",\n",
    "    \"CAT\",\n",
    "    \"CCA\",\n",
    "    \"CCC\",\n",
    "    \"CCG\",\n",
    "    \"CCT\",\n",
    "    \"CGA\",\n",
    "    \"CGC\",\n",
    "    \"CGG\",\n",
    "    \"CGT\",\n",
    "    \"CTA\",\n",
    "    \"CTC\",\n",
    "    \"CTG\",\n",
    "    \"CTT\",\n",
    "    \"GAA\",\n",
    "    \"GAC\",\n",
    "    \"GAG\",\n",
    "    \"GAT\",\n",
    "    \"GCA\",\n",
    "    \"GCC\",\n",
    "    \"GCG\",\n",
    "    \"GCT\",\n",
    "    \"GGA\",\n",
    "    \"GGC\",\n",
    "    \"GGG\",\n",
    "    \"GGT\",\n",
    "    \"GTA\",\n",
    "    \"GTC\",\n",
    "    \"GTG\",\n",
    "    \"GTT\",\n",
    "    \"TAA\",\n",
    "    \"TAC\",\n",
    "    \"TAG\",\n",
    "    \"TAT\",\n",
    "    \"TCA\",\n",
    "    \"TCC\",\n",
    "    \"TCG\",\n",
    "    \"TCT\",\n",
    "    \"TGA\",\n",
    "    \"TGC\",\n",
    "    \"TGG\",\n",
    "    \"TGT\",\n",
    "    \"TTA\",\n",
    "    \"TTC\",\n",
    "    \"TTG\",\n",
    "    \"TTT\",\n",
    "]\n",
    "\n",
    "\n",
    "tetra_nucleosides = [\n",
    "    \"AAAA\",\n",
    "    \"AAAC\",\n",
    "    \"AAAG\",\n",
    "    \"AAAT\",\n",
    "    \"AACA\",\n",
    "    \"AACC\",\n",
    "    \"AACG\",\n",
    "    \"AACT\",\n",
    "    \"AAGA\",\n",
    "    \"AAGC\",\n",
    "    \"AAGG\",\n",
    "    \"AAGT\",\n",
    "    \"AATA\",\n",
    "    \"AATC\",\n",
    "    \"AATG\",\n",
    "    \"AATT\",\n",
    "    \"ACAA\",\n",
    "    \"ACAC\",\n",
    "    \"ACAG\",\n",
    "    \"ACAT\",\n",
    "    \"ACCA\",\n",
    "    \"ACCC\",\n",
    "    \"ACCG\",\n",
    "    \"ACCT\",\n",
    "    \"ACGA\",\n",
    "    \"ACGC\",\n",
    "    \"ACGG\",\n",
    "    \"ACGT\",\n",
    "    \"ACTA\",\n",
    "    \"ACTC\",\n",
    "    \"ACTG\",\n",
    "    \"ACTT\",\n",
    "    \"AGAA\",\n",
    "    \"AGAC\",\n",
    "    \"AGAG\",\n",
    "    \"AGAT\",\n",
    "    \"AGCA\",\n",
    "    \"AGCC\",\n",
    "    \"AGCG\",\n",
    "    \"AGCT\",\n",
    "    \"AGGA\",\n",
    "    \"AGGC\",\n",
    "    \"AGGG\",\n",
    "    \"AGGT\",\n",
    "    \"AGTA\",\n",
    "    \"AGTC\",\n",
    "    \"AGTG\",\n",
    "    \"AGTT\",\n",
    "    \"ATAA\",\n",
    "    \"ATAC\",\n",
    "    \"ATAG\",\n",
    "    \"ATAT\",\n",
    "    \"ATCA\",\n",
    "    \"ATCC\",\n",
    "    \"ATCG\",\n",
    "    \"ATCT\",\n",
    "    \"ATGA\",\n",
    "    \"ATGC\",\n",
    "    \"ATGG\",\n",
    "    \"ATGT\",\n",
    "    \"ATTA\",\n",
    "    \"ATTC\",\n",
    "    \"ATTG\",\n",
    "    \"ATTT\",\n",
    "    \"CAAA\",\n",
    "    \"CAAC\",\n",
    "    \"CAAG\",\n",
    "    \"CAAT\",\n",
    "    \"CACA\",\n",
    "    \"CACC\",\n",
    "    \"CACG\",\n",
    "    \"CACT\",\n",
    "    \"CAGA\",\n",
    "    \"CAGC\",\n",
    "    \"CAGG\",\n",
    "    \"CAGT\",\n",
    "    \"CATA\",\n",
    "    \"CATC\",\n",
    "    \"CATG\",\n",
    "    \"CATT\",\n",
    "    \"CCAA\",\n",
    "    \"CCAC\",\n",
    "    \"CCAG\",\n",
    "    \"CCAT\",\n",
    "    \"CCCA\",\n",
    "    \"CCCC\",\n",
    "    \"CCCG\",\n",
    "    \"CCCT\",\n",
    "    \"CCGA\",\n",
    "    \"CCGC\",\n",
    "    \"CCGG\",\n",
    "    \"CCGT\",\n",
    "    \"CCTA\",\n",
    "    \"CCTC\",\n",
    "    \"CCTG\",\n",
    "    \"CCTT\",\n",
    "    \"CGAA\",\n",
    "    \"CGAC\",\n",
    "    \"CGAG\",\n",
    "    \"CGAT\",\n",
    "    \"CGCA\",\n",
    "    \"CGCC\",\n",
    "    \"CGCG\",\n",
    "    \"CGCT\",\n",
    "    \"CGGA\",\n",
    "    \"CGGC\",\n",
    "    \"CGGG\",\n",
    "    \"CGGT\",\n",
    "    \"CGTA\",\n",
    "    \"CGTC\",\n",
    "    \"CGTG\",\n",
    "    \"CGTT\",\n",
    "    \"CTAA\",\n",
    "    \"CTAC\",\n",
    "    \"CTAG\",\n",
    "    \"CTAT\",\n",
    "    \"CTCA\",\n",
    "    \"CTCC\",\n",
    "    \"CTCG\",\n",
    "    \"CTCT\",\n",
    "    \"CTGA\",\n",
    "    \"CTGC\",\n",
    "    \"CTGG\",\n",
    "    \"CTGT\",\n",
    "    \"CTTA\",\n",
    "    \"CTTC\",\n",
    "    \"CTTG\",\n",
    "    \"CTTT\",\n",
    "    \"GAAA\",\n",
    "    \"GAAC\",\n",
    "    \"GAAG\",\n",
    "    \"GAAT\",\n",
    "    \"GACA\",\n",
    "    \"GACC\",\n",
    "    \"GACG\",\n",
    "    \"GACT\",\n",
    "    \"GAGA\",\n",
    "    \"GAGC\",\n",
    "    \"GAGG\",\n",
    "    \"GAGT\",\n",
    "    \"GATA\",\n",
    "    \"GATC\",\n",
    "    \"GATG\",\n",
    "    \"GATT\",\n",
    "    \"GCAA\",\n",
    "    \"GCAC\",\n",
    "    \"GCAG\",\n",
    "    \"GCAT\",\n",
    "    \"GCCA\",\n",
    "    \"GCCC\",\n",
    "    \"GCCG\",\n",
    "    \"GCCT\",\n",
    "    \"GCGA\",\n",
    "    \"GCGC\",\n",
    "    \"GCGG\",\n",
    "    \"GCGT\",\n",
    "    \"GCTA\",\n",
    "    \"GCTC\",\n",
    "    \"GCTG\",\n",
    "    \"GCTT\",\n",
    "    \"GGAA\",\n",
    "    \"GGAC\",\n",
    "    \"GGAG\",\n",
    "    \"GGAT\",\n",
    "    \"GGCA\",\n",
    "    \"GGCC\",\n",
    "    \"GGCG\",\n",
    "    \"GGCT\",\n",
    "    \"GGGA\",\n",
    "    \"GGGC\",\n",
    "    \"GGGG\",\n",
    "    \"GGGT\",\n",
    "    \"GGTA\",\n",
    "    \"GGTC\",\n",
    "    \"GGTG\",\n",
    "    \"GGTT\",\n",
    "    \"GTAA\",\n",
    "    \"GTAC\",\n",
    "    \"GTAG\",\n",
    "    \"GTAT\",\n",
    "    \"GTCA\",\n",
    "    \"GTCC\",\n",
    "    \"GTCG\",\n",
    "    \"GTCT\",\n",
    "    \"GTGA\",\n",
    "    \"GTGC\",\n",
    "    \"GTGG\",\n",
    "    \"GTGT\",\n",
    "    \"GTTA\",\n",
    "    \"GTTC\",\n",
    "    \"GTTG\",\n",
    "    \"GTTT\",\n",
    "    \"TAAA\",\n",
    "    \"TAAC\",\n",
    "    \"TAAG\",\n",
    "    \"TAAT\",\n",
    "    \"TACA\",\n",
    "    \"TACC\",\n",
    "    \"TACG\",\n",
    "    \"TACT\",\n",
    "    \"TAGA\",\n",
    "    \"TAGC\",\n",
    "    \"TAGG\",\n",
    "    \"TAGT\",\n",
    "    \"TATA\",\n",
    "    \"TATC\",\n",
    "    \"TATG\",\n",
    "    \"TATT\",\n",
    "    \"TCAA\",\n",
    "    \"TCAC\",\n",
    "    \"TCAG\",\n",
    "    \"TCAT\",\n",
    "    \"TCCA\",\n",
    "    \"TCCC\",\n",
    "    \"TCCG\",\n",
    "    \"TCCT\",\n",
    "    \"TCGA\",\n",
    "    \"TCGC\",\n",
    "    \"TCGG\",\n",
    "    \"TCGT\",\n",
    "    \"TCTA\",\n",
    "    \"TCTC\",\n",
    "    \"TCTG\",\n",
    "    \"TCTT\",\n",
    "    \"TGAA\",\n",
    "    \"TGAC\",\n",
    "    \"TGAG\",\n",
    "    \"TGAT\",\n",
    "    \"TGCA\",\n",
    "    \"TGCC\",\n",
    "    \"TGCG\",\n",
    "    \"TGCT\",\n",
    "    \"TGGA\",\n",
    "    \"TGGC\",\n",
    "    \"TGGG\",\n",
    "    \"TGGT\",\n",
    "    \"TGTA\",\n",
    "    \"TGTC\",\n",
    "    \"TGTG\",\n",
    "    \"TGTT\",\n",
    "    \"TTAA\",\n",
    "    \"TTAC\",\n",
    "    \"TTAG\",\n",
    "    \"TTAT\",\n",
    "    \"TTCA\",\n",
    "    \"TTCC\",\n",
    "    \"TTCG\",\n",
    "    \"TTCT\",\n",
    "    \"TTGA\",\n",
    "    \"TTGC\",\n",
    "    \"TTGG\",\n",
    "    \"TTGT\",\n",
    "    \"TTTA\",\n",
    "    \"TTTC\",\n",
    "    \"TTTG\",\n",
    "    \"TTTT\",\n",
    "]\n",
    "\n",
    "\n",
    "def dump(value=None, filename=None):\n",
    "    if (value is not None) and (filename is not None):\n",
    "        joblib.dump(value=value, filename=filename)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Value & Filename should be passed.\".capitalize())\n",
    "\n",
    "\n",
    "def load(filename=None):\n",
    "    if filename is not None:\n",
    "        return joblib.load(filename=filename)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Filename should be passed in an appropriate manner\".capitalize()\n",
    "        )\n",
    "\n",
    "\n",
    "def config():\n",
    "    with open(\"../config.yml\", \"r\") as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "\n",
    "def hyperparameter_tuning(model: str = \"RF\"):\n",
    "    if model == \"RF\":\n",
    "        return {\n",
    "            \"n_estimators\": [100, 200, 300],\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        }\n",
    "    elif model == \"DT\":\n",
    "        return {\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"max_depth\": [None, 10, 20, 30],\n",
    "            \"min_samples_split\": [2, 5],\n",
    "            \"min_samples_leaf\": [1, 2],\n",
    "        }\n",
    "    elif model == \"LR\":\n",
    "        return {\n",
    "            \"penalty\": [\"l1\", \"l2\", \"elasticnet\", \"none\"],\n",
    "            \"C\": [0.001, 0.01, 0.1, 1, 10],\n",
    "            \"max_iter\": [100, 200, 300],\n",
    "        }\n",
    "    elif model == \"XGB\":\n",
    "        return {\n",
    "            \"learning_rate\": [0.01, 0.1, 1],\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "            \"n_estimators\": [100, 200, 300],\n",
    "        }\n",
    "    elif model == \"NB\":\n",
    "        return {\n",
    "            \"var_smoothing\": [1e-09],\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"The model name is not supported. Please check the model name and try again\".capitalize()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generator for DNA-Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class FeatureGenerator:\n",
    "    def __init__(\n",
    "        self, approaches: list = [\"single\", \"di\", \"tri\", \"tetra\", \"gc-content\"]\n",
    "    ):\n",
    "        self.approaches = approaches\n",
    "\n",
    "        self.X = list()\n",
    "        self.y = list()\n",
    "\n",
    "        self.GC_Content = list()\n",
    "\n",
    "        self.dataset = pd.read_csv(\"../data/raw/DNA-Classification.csv\")[0:5] # I am using sub sample as it would take a huge time to craete\n",
    "\n",
    "    def feature_generator(self):\n",
    "        if \"single\" in self.approaches:\n",
    "            max_len = max(self.dataset[\"sequence\"].apply(len))\n",
    "\n",
    "            for pos in range(max_len):\n",
    "                for nucleoside in single_nucleosides:\n",
    "                    feature_column = f\"{nucleoside}_pos_{pos}\"\n",
    "                    self.dataset[feature_column] = 0\n",
    "\n",
    "            for instance in tqdm(range(self.dataset.shape[0])):\n",
    "                sequence = self.dataset.loc[instance, \"sequence\"]\n",
    "\n",
    "                for pos, nucleotide in enumerate(sequence):\n",
    "                    for nucleoside in single_nucleosides:\n",
    "                        feature_column = f\"{nucleoside}_pos_{pos}\"\n",
    "                        if nucleoside == nucleotide:\n",
    "                            self.dataset.loc[instance, feature_column] = 1\n",
    "\n",
    "        if \"di\" in self.approaches:\n",
    "            max_len = max(self.dataset[\"sequence\"].apply(len))\n",
    "\n",
    "            for pos in range(max_len - 1):\n",
    "                for di_nucleoside in di_nucleosides:\n",
    "                    feature_column = f\"{di_nucleoside}_pos_{pos}_di_nucleoside\"\n",
    "                    self.dataset[feature_column] = 0\n",
    "\n",
    "            for instance in tqdm(range(self.dataset.shape[0])):\n",
    "                sequence = self.dataset.loc[instance, \"sequence\"]\n",
    "                for pos in range(len(sequence) - 1):\n",
    "                    for di_nucleoside in di_nucleosides:\n",
    "                        feature_column = f\"{di_nucleoside}_pos_{pos}_di_nucleoside\"\n",
    "                        if sequence[pos : pos + 2] == di_nucleoside:\n",
    "                            self.dataset.loc[instance, feature_column] = 1\n",
    "\n",
    "        if \"tri\" in self.approaches:\n",
    "            max_len = max(self.dataset[\"sequence\"].apply(len))\n",
    "\n",
    "            for pos in range(max_len - 2):\n",
    "                for tri_nucleoside in tri_nucleosides:\n",
    "                    feature_column = f\"{tri_nucleoside}_pos_{pos}_tri_nucleoside\"\n",
    "                    self.dataset[feature_column] = 0\n",
    "\n",
    "            for instance in tqdm(range(self.dataset.shape[0])):\n",
    "                sequence = self.dataset.loc[instance, \"sequence\"]\n",
    "                for pos in range(len(sequence) - 2):\n",
    "                    for tri_nucleoside in tri_nucleosides:\n",
    "                        feature_column = f\"{tri_nucleoside}_pos_{pos}_tri_nucleoside\"\n",
    "                        if sequence[pos : pos + 3] == tri_nucleoside:\n",
    "                            self.dataset.loc[instance, feature_column] = 1\n",
    "\n",
    "        if \"tetra\" in self.approaches:\n",
    "            max_len = max(self.dataset[\"sequence\"].apply(len))\n",
    "\n",
    "            for pos in range(max_len - 3):\n",
    "                for tetra_nucleoside in tetra_nucleosides:\n",
    "                    feature_column = f\"{tetra_nucleoside}_pos_{pos}_tetra_nucleoside\"\n",
    "                    self.dataset[feature_column] = 0\n",
    "\n",
    "            for instance in tqdm(range(self.dataset.shape[0])):\n",
    "                sequence = self.dataset.loc[instance, \"sequence\"]\n",
    "                for pos in range(len(sequence) - 3):\n",
    "                    for tetra_nucleoside in tetra_nucleosides:\n",
    "                        feature_column = (\n",
    "                            f\"{tetra_nucleoside}_pos_{pos}_tetra_nucleoside\"\n",
    "                        )\n",
    "                        if sequence[pos : pos + 4] == tetra_nucleoside:\n",
    "                            self.dataset.loc[instance, feature_column] = 1\n",
    "\n",
    "        if \"gc-content\" in self.approaches:\n",
    "            self.GC_Content = []\n",
    "\n",
    "            for instance in tqdm(range(self.dataset.shape[0])):\n",
    "                sequence = self.dataset.loc[instance, \"sequence\"]\n",
    "                G_count = sequence.count(\"G\")\n",
    "                C_count = sequence.count(\"C\")\n",
    "                GC_Content = (\n",
    "                    (G_count + C_count) / len(sequence) if len(sequence) > 0 else 0\n",
    "                )\n",
    "                self.GC_Content.append(GC_Content)\n",
    "\n",
    "            self.dataset[\"GC-Content\"] = self.GC_Content\n",
    "\n",
    "        try:\n",
    "            self.dataset.to_csv(\n",
    "                os.path.join(\n",
    "                    config()[\"path\"][\"processed_path\"], \"processed_dataset.csv\"\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                \"Cannot saved the dataset in the processed file, & error: {}\".capitalize().format(\n",
    "                    e\n",
    "                )\n",
    "            )\n",
    "            traceback.print_exc()\n",
    "        else:\n",
    "            print(\n",
    "                \"the dataset stored in the {} folder\".format(\n",
    "                    config()[\"path\"][\"processed_path\"]\n",
    "                ).capitalize()\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generator = FeatureGenerator(approaches=[\"single\"])\n",
    "    generator.feature_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset=None,\n",
    "        split_size: float = 0.20,\n",
    "        approaches: list = [\"single\", \"di\", \"tri\", \"tetra\", \"gc_content\"],\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.split_size = split_size\n",
    "        self.approaches = approaches\n",
    "\n",
    "    def split_dataset(self):\n",
    "        if os.path.exists(config()[\"path\"][\"processed_path\"]):\n",
    "            dataset = os.path.join(\n",
    "                config()[\"path\"][\"processed_path\"], \"processed_dataset.csv\"\n",
    "            )\n",
    "\n",
    "            self.processed_data = pd.read_csv(dataset)\n",
    "\n",
    "            X = self.processed_data.iloc[:, 4:]\n",
    "            y = self.processed_data.iloc[:, 3]\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=self.split_size, random_state=42\n",
    "            )\n",
    "\n",
    "            for type, dataset in [\n",
    "                (\"X_train\", X_train),\n",
    "                (\"X_test\", X_test),\n",
    "                (\"y_train\", y_train),\n",
    "                (\"y_test\", y_test),\n",
    "            ]:\n",
    "                dataset.to_csv(\n",
    "                    os.path.join(config()[\"path\"][\"processed_path\"], f\"{type}.csv\"),\n",
    "                    index=False,\n",
    "                )\n",
    "\n",
    "            print(\n",
    "                \"Training and testing dataset is stored in the folder {}\".format(\n",
    "                    config()[\"path\"][\"processed_path\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"X_train\": X_train,\n",
    "                \"X_test\": X_test,\n",
    "                \"y_train\": y_train,\n",
    "                \"y_test\": y_test,\n",
    "            }\n",
    "\n",
    "    def feature_generator(self):\n",
    "        if isinstance(self.approaches, list):\n",
    "            self.generator = FeatureGenerator(approaches=self.approaches)\n",
    "\n",
    "            try:\n",
    "                self.generator.feature_generator()\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\".capitalize())\n",
    "            else:\n",
    "                print(\n",
    "                    \"Feature generation completed successfully and store in the folder {}\".format(\n",
    "                        os.path.join(config()[\"path\"][\"processed_path\"])\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError(\"Approaches must be a list\".capitalize())\n",
    "\n",
    "    @staticmethod\n",
    "    def dataset_history():\n",
    "        if os.path.exists(config()[\"path\"][\"processed_path\"]):\n",
    "            processed_path = os.path.join(\n",
    "                config()[\"path\"][\"processed_path\"], \"processed_dataset.csv\"\n",
    "            )\n",
    "\n",
    "            dataset = pd.read_csv(processed_path)\n",
    "\n",
    "            information = {}\n",
    "\n",
    "            information[\"isNaN\".title()] = (\n",
    "                \"NaN\".capitalize()\n",
    "                if dataset.isnull().sum().sum() > 0\n",
    "                else \"no NaN\".capitalize()\n",
    "            )\n",
    "\n",
    "            information[\"total_features\".title()] = str(dataset.shape[1])\n",
    "            information[\"total_instances\".title()] = str(dataset.shape[0])\n",
    "            information[\"dataset_shape\".title()] = str(dataset.shape)\n",
    "            information[\"target_ratio\".title()] = str(\n",
    "                dataset[\"labels\"].value_counts(ascending=False).to_dict()\n",
    "            )\n",
    "\n",
    "            pd.DataFrame(information, index=[0]).to_csv(\n",
    "                os.path.join(config()[\"path\"][\"files_path\"], \"dataset_history.csv\")\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                \"Dataset history is stored in the folder {}\".format(\n",
    "                    config()[\"path\"][\"files_path\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = DataLoader(\n",
    "        dataset=\"../data/raw/DNA-Classification.csv\",\n",
    "        split_size=0.2,\n",
    "        approaches=[\"single\"]  # please use \"di\", \"tri\", \"tetra\" and GC-content so create a huge amount of features\n",
    "    )\n",
    "    \n",
    "    loader.feature_generator()\n",
    "    \n",
    "    DataLoader.dataset_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the MLModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineLearningModel:\n",
    "    def __init__(self, model: str = \"RF\"):\n",
    "        self.model = model\n",
    "\n",
    "    def define_model(self):\n",
    "        if self.model == \"RF\":\n",
    "            return RandomForestClassifier()\n",
    "        elif self.model == \"DT\":\n",
    "            return DecisionTreeClassifier()\n",
    "        elif self.model == \"NB\":\n",
    "            return MultinomialNB()\n",
    "        elif self.model == \"LR\":\n",
    "            return LogisticRegression()\n",
    "        elif self.model == \"XGB\":\n",
    "            return XGBClassifier()\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Select the appropriate machine learning model to train the model\".capitalize()\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Define the model for the DNA-Classification\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        default=config()[\"model\"][\"model_name\"],\n",
    "        choices=[\"RF\", \"DT\", \"NB\", \"LR\", \"XGB\"],\n",
    "        help=\"Define the model name for the model\".capitalize(),\n",
    "    )\n",
    "\n",
    "    model = MachineLearningModel()\n",
    "    model.define_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_initialization():\n",
    "\n",
    "    X_train = pd.read_csv(\n",
    "        os.path.join(config()[\"path\"][\"processed_path\"], \"X_train.csv\")\n",
    "    )\n",
    "    X_test = pd.read_csv(\n",
    "        os.path.join(config()[\"path\"][\"processed_path\"], \"X_test.csv\"),\n",
    "    )\n",
    "    y_train = pd.read_csv(\n",
    "        os.path.join(config()[\"path\"][\"processed_path\"], \"y_train.csv\"),\n",
    "    )\n",
    "    y_test = pd.read_csv(\n",
    "        os.path.join(config()[\"path\"][\"processed_path\"], \"y_test.csv\"),\n",
    "    )\n",
    "\n",
    "    training_dataset = pd.concat(\n",
    "        [X_train, y_train],\n",
    "        axis=1,\n",
    "    )\n",
    "    testing_dataset = pd.concat(\n",
    "        [X_test, y_test],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    training_dataset = training_dataset.iloc[:, :-1]\n",
    "    testing_dataset = testing_dataset.iloc[:, :-1]\n",
    "\n",
    "    return {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test,\n",
    "        \"training_dataset\": training_dataset,\n",
    "        \"testing_dataset\": testing_dataset,\n",
    "    }\n",
    "\n",
    "\n",
    "def features_extraction_technique():\n",
    "    try:\n",
    "        dataset = dataset_initialization()\n",
    "\n",
    "        training_dataset = dataset[\"training_dataset\"]\n",
    "        testing_dataset = dataset[\"testing_dataset\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error is occured: \", e)\n",
    "\n",
    "    try:\n",
    "        pca = PCA()\n",
    "        pca.fit(training_dataset)\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(\"An error is occured: \", e)\n",
    "    except Exception as e:\n",
    "        print(\"An error is occured: \", e)\n",
    "\n",
    "    explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    best_n_components = np.argmax(explained_variance >= 0.90) + 1\n",
    "\n",
    "    pca = PCA(n_components=best_n_components)\n",
    "\n",
    "    X_train_transformed = pca.fit_transform(training_dataset)\n",
    "    X_test_transformed = pca.transform(testing_dataset)\n",
    "\n",
    "    X_train_df = pd.DataFrame(\n",
    "        X_train_transformed,\n",
    "        index=dataset[\"X_train\"].index,\n",
    "    )\n",
    "    X_test_df = pd.DataFrame(\n",
    "        X_test_transformed,\n",
    "        index=dataset[\"X_test\"].index,\n",
    "    )\n",
    "\n",
    "    y_train = dataset[\"y_train\"].reset_index(drop=True)[\"labels\"]\n",
    "    y_test = dataset[\"y_test\"].reset_index(drop=True)[\"labels\"]\n",
    "\n",
    "    dataset = pd.concat(\n",
    "        [\n",
    "            pd.concat([X_train_df, y_train], axis=1),\n",
    "            pd.concat([X_test_df, y_test], axis=1),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    X = dataset.drop(\"labels\", axis=1)\n",
    "    y = dataset[\"labels\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=config()[\"dataloader\"][\"split_size\"],\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    os.makedirs(\n",
    "        os.path.join(config()[\"path\"][\"processed_path\"], \"PCA-dataset\"), exist_ok=True\n",
    "    )\n",
    "\n",
    "    for dataset_name, data in [\n",
    "        (\"X_train\", X_train),\n",
    "        (\"X_test\", X_test),\n",
    "        (\"y_train\", y_train),\n",
    "        (\"y_test\", y_test),\n",
    "    ]:\n",
    "        data.to_csv(\n",
    "            os.path.join(\n",
    "                config()[\"path\"][\"processed_path\"], \"PCA-dataset\", f\"{dataset_name}.csv\"\n",
    "            ),\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "    return {\"X_train\": X_train, \"X_test\": X_test, \"y_train\": y_train, \"y_test\": y_test}\n",
    "\n",
    "\n",
    "def features_selection_technique():\n",
    "    RF = RandomForestClassifier(n_estimators=300, criterion=\"gini\", random_state=42)\n",
    "\n",
    "    try:\n",
    "        dataset = dataset_initialization()\n",
    "\n",
    "        X_train = dataset[\"X_train\"]\n",
    "        y_train = dataset[\"y_train\"]\n",
    "        X_test = dataset[\"X_test\"]\n",
    "        y_test = dataset[\"y_test\"]\n",
    "\n",
    "        RF.fit(X_train, y_train)\n",
    "\n",
    "        feature_importances = RF.feature_importances_\n",
    "\n",
    "        importance_df = pd.concat(\n",
    "            [\n",
    "                pd.DataFrame(X_train.columns, columns=[\"Features\"]),\n",
    "                pd.DataFrame(feature_importances, columns=[\"Importance\"]),\n",
    "            ],\n",
    "            axis=1,\n",
    "        ).sort_values(by=[\"Importance\"], ascending=False)\n",
    "\n",
    "        columns = importance_df[importance_df[\"Importance\"] >= 0.001][\"Features\"].values\n",
    "        index = importance_df[importance_df[\"Importance\"] >= 0.001].index\n",
    "\n",
    "        X_train = X_train.loc[:, columns]\n",
    "        X_test = X_test.loc[:, columns]\n",
    "\n",
    "        os.makedirs(\n",
    "            os.path.join(config()[\"path\"][\"processed_path\"], \"Feature-Importance\"),\n",
    "            exist_ok=True,\n",
    "        )\n",
    "\n",
    "        for dataset_name, data in [\n",
    "            (\"X_train\", X_train),\n",
    "            (\"X_test\", X_test),\n",
    "            (\"y_train\", y_train),\n",
    "            (\"y_test\", y_test),\n",
    "        ]:\n",
    "            data.to_csv(\n",
    "                os.path.join(\n",
    "                    config()[\"path\"][\"processed_path\"],\n",
    "                    \"Feature-Importance\",\n",
    "                    f\"{dataset_name}.csv\",\n",
    "                ),\n",
    "                index=False,\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\": y_train,\n",
    "            \"y_test\": y_test,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred: \", e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Helper method for the DNA-Classifier\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--FE\",\n",
    "        type=str,\n",
    "        default=\"PCA\",\n",
    "        help=\"Features Extraction Technique\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--FS\",\n",
    "        type=bool,\n",
    "        default=False,\n",
    "        help=\"Features Selection Technique\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.FE:\n",
    "        _ = features_extraction_technique()\n",
    "    elif args.FS:\n",
    "        _ = features_selection_technique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"RF\",\n",
    "        features_extraction: bool = False,\n",
    "        features_selection: bool = False,\n",
    "        hyperparameter_tuning: bool = False,\n",
    "        KFold: int = 5,\n",
    "    ):\n",
    "        self.steps = \"training\"\n",
    "        self.model = model\n",
    "        self.features_extraction = features_extraction\n",
    "        self.features_selection = features_selection\n",
    "        self.hyperparameter_tuning = hyperparameter_tuning\n",
    "        self.KFold = KFold\n",
    "\n",
    "        self.accuracy = []\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.f1_score = []\n",
    "\n",
    "    def choose_dataset(self):\n",
    "        if self.features_extraction:\n",
    "            return features_extraction_technique()\n",
    "        elif self.features_selection:\n",
    "            return features_selection_technique()\n",
    "        else:\n",
    "            path = config()[\"path\"][\"processed_data\"]\n",
    "            return (\n",
    "                {\n",
    "                    \"X_train\": os.path.join(path, \"X_train.csv\"),\n",
    "                    \"X_test\": os.path.join(path, \"X_test.csv\"),\n",
    "                    \"y_train\": os.path.join(path, \"y_train.csv\"),\n",
    "                    \"y_test\": os.path.join(path, \"y_test.csv\"),\n",
    "                }\n",
    "                if os.path.exists(config()[\"path\"][\"processed_data\"])\n",
    "                else \"Make sure the processed data is in the right path\".capitalize()\n",
    "            )\n",
    "\n",
    "    def select_the_model(self):\n",
    "        if self.model == \"RF\":\n",
    "            return MachineLearningModel(model=\"RF\").define_model()\n",
    "        elif self.model == \"DT\":\n",
    "            return MachineLearningModel(model=\"DT\").define_model()\n",
    "        elif self.model == \"LR\":\n",
    "            return MachineLearningModel(model=\"LR\").define_model()\n",
    "        elif self.model == \"XGB\":\n",
    "            return MachineLearningModel(model=\"XGB\").define_model()\n",
    "        elif self.model == \"NB\":\n",
    "            return MachineLearningModel(model=\"NB\").define_model()\n",
    "        else:\n",
    "            return \"Make sure the model is in the right format\".capitalize()\n",
    "\n",
    "    def model_evaluation(self, **kwargs):\n",
    "        with open(\"./evaluation.json\", \"w\") as file:\n",
    "            json.dump(\n",
    "                {\n",
    "                    \"Accuracy\": np.mean(kwargs[\"accuracy\"]).round(2),\n",
    "                    \"Precision\": np.mean(kwargs[\"precision\"]).round(2),\n",
    "                    \"Recall\": np.mean(kwargs[\"recall\"]).round(2),\n",
    "                    \"F1 Score\": np.mean(kwargs[\"f1_score\"]).round(2),\n",
    "                    \"Classification Report\": classification_report(\n",
    "                        kwargs[\"actual_labels\"],\n",
    "                        kwargs[\"predicted_labels\"],\n",
    "                        output_dict=True,\n",
    "                    ),\n",
    "                },\n",
    "                file,\n",
    "                indent=4,\n",
    "            )\n",
    "\n",
    "    def train(self):\n",
    "        dataset = self.choose_dataset()\n",
    "        classifier = self.select_the_model()\n",
    "\n",
    "        if self.hyperparameter_tuning:\n",
    "            classifier = GridSearchCV(\n",
    "                estimator=classifier,\n",
    "                param_grid=hyperparameter_tuning(model=self.model),\n",
    "                cv=self.KFold,\n",
    "                scoring=\"accuracy\",\n",
    "            )\n",
    "\n",
    "            classifier.fit(dataset[\"X_train\"], dataset[\"y_train\"])\n",
    "\n",
    "            predicted = classifier.predict(dataset[\"X_test\"])\n",
    "\n",
    "            print(\"The best parameters are: \".capitalize(), classifier.best_params_)\n",
    "            print(\"Refined best parameters: \".capitalize(), classifier.best_score_)\n",
    "\n",
    "            self.model_evaluation(\n",
    "                accuracy=accuracy_score(predicted, dataset[\"y_test\"]),\n",
    "                precision=precision_score(\n",
    "                    predicted, dataset[\"y_test\"], average=\"weighted\"\n",
    "                ),\n",
    "                recall=recall_score(predicted, dataset[\"y_test\"], average=\"weighted\"),\n",
    "                f1_score=f1_score(predicted, dataset[\"y_test\"], average=\"weighted\"),\n",
    "                actual_labels=dataset[\"y_test\"],\n",
    "                predicted_labels=predicted,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            predicted_labels = []\n",
    "            actual_labels = []\n",
    "\n",
    "            KFoldCV = KFold(n_splits=self.KFold, shuffle=True, random_state=42)\n",
    "\n",
    "            X = pd.concat([dataset[\"X_train\"], dataset[\"X_test\"]], axis=0).reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "            y = pd.concat([dataset[\"y_train\"], dataset[\"y_test\"]], axis=0).reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "\n",
    "            for index, (train_index, test_index) in enumerate(KFoldCV.split(X, y)):\n",
    "                print(f\"{'*' * 10} KFold CV - {index + 1} is executing {'*' * 10}\")\n",
    "\n",
    "                X_train_fold, y_train_fold = X.iloc[train_index, :], y.iloc[train_index]\n",
    "                X_test_fold, y_test_fold = X.iloc[test_index, :], y.iloc[test_index]\n",
    "\n",
    "                classifier.fit(X_train_fold, y_train_fold)\n",
    "                predicted = classifier.predict(X_test_fold)\n",
    "\n",
    "                self.accuracy.append(accuracy_score(y_test_fold, predicted))\n",
    "                self.precision.append(\n",
    "                    precision_score(y_test_fold, predicted, average=\"weighted\")\n",
    "                )\n",
    "                self.recall.append(\n",
    "                    recall_score(y_test_fold, predicted, average=\"weighted\")\n",
    "                )\n",
    "                self.f1_score.append(\n",
    "                    f1_score(y_test_fold, predicted, average=\"weighted\")\n",
    "                )\n",
    "\n",
    "                predicted_labels.extend(predicted)\n",
    "                actual_labels.extend(y_test_fold.values.ravel())\n",
    "\n",
    "            self.model_evaluation(\n",
    "                accuracy=self.accuracy,\n",
    "                precision=self.precision,\n",
    "                recall=self.recall,\n",
    "                f1_score=self.f1_score,\n",
    "                predicted_labels=predicted_labels,\n",
    "                actual_labels=actual_labels,\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                \"The evaluation metrics are saved in the evaluation.json file\".capitalize()\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainer = Trainer(\n",
    "        features_extraction=True, hyperparameter_tuning=False, model=\"RF\", KFold=2\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
