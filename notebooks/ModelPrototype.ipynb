{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import joblib\n",
    "import argparse\n",
    "import warnings\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_nucleosides = [\"A\", \"C\", \"G\", \"T\"]\n",
    "\n",
    "di_nucleosides = [\n",
    "    \"AA\",\n",
    "    \"AC\",\n",
    "    \"AG\",\n",
    "    \"AT\",\n",
    "    \"CA\",\n",
    "    \"CC\",\n",
    "    \"CG\",\n",
    "    \"CT\",\n",
    "    \"GA\",\n",
    "    \"GC\",\n",
    "    \"GG\",\n",
    "    \"GT\",\n",
    "    \"TA\",\n",
    "    \"TC\",\n",
    "    \"TG\",\n",
    "    \"TT\",\n",
    "]\n",
    "\n",
    "tri_nucleosides = [\n",
    "    \"AAA\",\n",
    "    \"AAC\",\n",
    "    \"AAG\",\n",
    "    \"AAT\",\n",
    "    \"ACA\",\n",
    "    \"ACC\",\n",
    "    \"ACG\",\n",
    "    \"ACT\",\n",
    "    \"AGA\",\n",
    "    \"AGC\",\n",
    "    \"AGG\",\n",
    "    \"AGT\",\n",
    "    \"ATA\",\n",
    "    \"ATC\",\n",
    "    \"ATG\",\n",
    "    \"ATT\",\n",
    "    \"CAA\",\n",
    "    \"CAC\",\n",
    "    \"CAG\",\n",
    "    \"CAT\",\n",
    "    \"CCA\",\n",
    "    \"CCC\",\n",
    "    \"CCG\",\n",
    "    \"CCT\",\n",
    "    \"CGA\",\n",
    "    \"CGC\",\n",
    "    \"CGG\",\n",
    "    \"CGT\",\n",
    "    \"CTA\",\n",
    "    \"CTC\",\n",
    "    \"CTG\",\n",
    "    \"CTT\",\n",
    "    \"GAA\",\n",
    "    \"GAC\",\n",
    "    \"GAG\",\n",
    "    \"GAT\",\n",
    "    \"GCA\",\n",
    "    \"GCC\",\n",
    "    \"GCG\",\n",
    "    \"GCT\",\n",
    "    \"GGA\",\n",
    "    \"GGC\",\n",
    "    \"GGG\",\n",
    "    \"GGT\",\n",
    "    \"GTA\",\n",
    "    \"GTC\",\n",
    "    \"GTG\",\n",
    "    \"GTT\",\n",
    "    \"TAA\",\n",
    "    \"TAC\",\n",
    "    \"TAG\",\n",
    "    \"TAT\",\n",
    "    \"TCA\",\n",
    "    \"TCC\",\n",
    "    \"TCG\",\n",
    "    \"TCT\",\n",
    "    \"TGA\",\n",
    "    \"TGC\",\n",
    "    \"TGG\",\n",
    "    \"TGT\",\n",
    "    \"TTA\",\n",
    "    \"TTC\",\n",
    "    \"TTG\",\n",
    "    \"TTT\",\n",
    "]\n",
    "\n",
    "\n",
    "tetra_nucleosides = [\n",
    "    \"AAAA\",\n",
    "    \"AAAC\",\n",
    "    \"AAAG\",\n",
    "    \"AAAT\",\n",
    "    \"AACA\",\n",
    "    \"AACC\",\n",
    "    \"AACG\",\n",
    "    \"AACT\",\n",
    "    \"AAGA\",\n",
    "    \"AAGC\",\n",
    "    \"AAGG\",\n",
    "    \"AAGT\",\n",
    "    \"AATA\",\n",
    "    \"AATC\",\n",
    "    \"AATG\",\n",
    "    \"AATT\",\n",
    "    \"ACAA\",\n",
    "    \"ACAC\",\n",
    "    \"ACAG\",\n",
    "    \"ACAT\",\n",
    "    \"ACCA\",\n",
    "    \"ACCC\",\n",
    "    \"ACCG\",\n",
    "    \"ACCT\",\n",
    "    \"ACGA\",\n",
    "    \"ACGC\",\n",
    "    \"ACGG\",\n",
    "    \"ACGT\",\n",
    "    \"ACTA\",\n",
    "    \"ACTC\",\n",
    "    \"ACTG\",\n",
    "    \"ACTT\",\n",
    "    \"AGAA\",\n",
    "    \"AGAC\",\n",
    "    \"AGAG\",\n",
    "    \"AGAT\",\n",
    "    \"AGCA\",\n",
    "    \"AGCC\",\n",
    "    \"AGCG\",\n",
    "    \"AGCT\",\n",
    "    \"AGGA\",\n",
    "    \"AGGC\",\n",
    "    \"AGGG\",\n",
    "    \"AGGT\",\n",
    "    \"AGTA\",\n",
    "    \"AGTC\",\n",
    "    \"AGTG\",\n",
    "    \"AGTT\",\n",
    "    \"ATAA\",\n",
    "    \"ATAC\",\n",
    "    \"ATAG\",\n",
    "    \"ATAT\",\n",
    "    \"ATCA\",\n",
    "    \"ATCC\",\n",
    "    \"ATCG\",\n",
    "    \"ATCT\",\n",
    "    \"ATGA\",\n",
    "    \"ATGC\",\n",
    "    \"ATGG\",\n",
    "    \"ATGT\",\n",
    "    \"ATTA\",\n",
    "    \"ATTC\",\n",
    "    \"ATTG\",\n",
    "    \"ATTT\",\n",
    "    \"CAAA\",\n",
    "    \"CAAC\",\n",
    "    \"CAAG\",\n",
    "    \"CAAT\",\n",
    "    \"CACA\",\n",
    "    \"CACC\",\n",
    "    \"CACG\",\n",
    "    \"CACT\",\n",
    "    \"CAGA\",\n",
    "    \"CAGC\",\n",
    "    \"CAGG\",\n",
    "    \"CAGT\",\n",
    "    \"CATA\",\n",
    "    \"CATC\",\n",
    "    \"CATG\",\n",
    "    \"CATT\",\n",
    "    \"CCAA\",\n",
    "    \"CCAC\",\n",
    "    \"CCAG\",\n",
    "    \"CCAT\",\n",
    "    \"CCCA\",\n",
    "    \"CCCC\",\n",
    "    \"CCCG\",\n",
    "    \"CCCT\",\n",
    "    \"CCGA\",\n",
    "    \"CCGC\",\n",
    "    \"CCGG\",\n",
    "    \"CCGT\",\n",
    "    \"CCTA\",\n",
    "    \"CCTC\",\n",
    "    \"CCTG\",\n",
    "    \"CCTT\",\n",
    "    \"CGAA\",\n",
    "    \"CGAC\",\n",
    "    \"CGAG\",\n",
    "    \"CGAT\",\n",
    "    \"CGCA\",\n",
    "    \"CGCC\",\n",
    "    \"CGCG\",\n",
    "    \"CGCT\",\n",
    "    \"CGGA\",\n",
    "    \"CGGC\",\n",
    "    \"CGGG\",\n",
    "    \"CGGT\",\n",
    "    \"CGTA\",\n",
    "    \"CGTC\",\n",
    "    \"CGTG\",\n",
    "    \"CGTT\",\n",
    "    \"CTAA\",\n",
    "    \"CTAC\",\n",
    "    \"CTAG\",\n",
    "    \"CTAT\",\n",
    "    \"CTCA\",\n",
    "    \"CTCC\",\n",
    "    \"CTCG\",\n",
    "    \"CTCT\",\n",
    "    \"CTGA\",\n",
    "    \"CTGC\",\n",
    "    \"CTGG\",\n",
    "    \"CTGT\",\n",
    "    \"CTTA\",\n",
    "    \"CTTC\",\n",
    "    \"CTTG\",\n",
    "    \"CTTT\",\n",
    "    \"GAAA\",\n",
    "    \"GAAC\",\n",
    "    \"GAAG\",\n",
    "    \"GAAT\",\n",
    "    \"GACA\",\n",
    "    \"GACC\",\n",
    "    \"GACG\",\n",
    "    \"GACT\",\n",
    "    \"GAGA\",\n",
    "    \"GAGC\",\n",
    "    \"GAGG\",\n",
    "    \"GAGT\",\n",
    "    \"GATA\",\n",
    "    \"GATC\",\n",
    "    \"GATG\",\n",
    "    \"GATT\",\n",
    "    \"GCAA\",\n",
    "    \"GCAC\",\n",
    "    \"GCAG\",\n",
    "    \"GCAT\",\n",
    "    \"GCCA\",\n",
    "    \"GCCC\",\n",
    "    \"GCCG\",\n",
    "    \"GCCT\",\n",
    "    \"GCGA\",\n",
    "    \"GCGC\",\n",
    "    \"GCGG\",\n",
    "    \"GCGT\",\n",
    "    \"GCTA\",\n",
    "    \"GCTC\",\n",
    "    \"GCTG\",\n",
    "    \"GCTT\",\n",
    "    \"GGAA\",\n",
    "    \"GGAC\",\n",
    "    \"GGAG\",\n",
    "    \"GGAT\",\n",
    "    \"GGCA\",\n",
    "    \"GGCC\",\n",
    "    \"GGCG\",\n",
    "    \"GGCT\",\n",
    "    \"GGGA\",\n",
    "    \"GGGC\",\n",
    "    \"GGGG\",\n",
    "    \"GGGT\",\n",
    "    \"GGTA\",\n",
    "    \"GGTC\",\n",
    "    \"GGTG\",\n",
    "    \"GGTT\",\n",
    "    \"GTAA\",\n",
    "    \"GTAC\",\n",
    "    \"GTAG\",\n",
    "    \"GTAT\",\n",
    "    \"GTCA\",\n",
    "    \"GTCC\",\n",
    "    \"GTCG\",\n",
    "    \"GTCT\",\n",
    "    \"GTGA\",\n",
    "    \"GTGC\",\n",
    "    \"GTGG\",\n",
    "    \"GTGT\",\n",
    "    \"GTTA\",\n",
    "    \"GTTC\",\n",
    "    \"GTTG\",\n",
    "    \"GTTT\",\n",
    "    \"TAAA\",\n",
    "    \"TAAC\",\n",
    "    \"TAAG\",\n",
    "    \"TAAT\",\n",
    "    \"TACA\",\n",
    "    \"TACC\",\n",
    "    \"TACG\",\n",
    "    \"TACT\",\n",
    "    \"TAGA\",\n",
    "    \"TAGC\",\n",
    "    \"TAGG\",\n",
    "    \"TAGT\",\n",
    "    \"TATA\",\n",
    "    \"TATC\",\n",
    "    \"TATG\",\n",
    "    \"TATT\",\n",
    "    \"TCAA\",\n",
    "    \"TCAC\",\n",
    "    \"TCAG\",\n",
    "    \"TCAT\",\n",
    "    \"TCCA\",\n",
    "    \"TCCC\",\n",
    "    \"TCCG\",\n",
    "    \"TCCT\",\n",
    "    \"TCGA\",\n",
    "    \"TCGC\",\n",
    "    \"TCGG\",\n",
    "    \"TCGT\",\n",
    "    \"TCTA\",\n",
    "    \"TCTC\",\n",
    "    \"TCTG\",\n",
    "    \"TCTT\",\n",
    "    \"TGAA\",\n",
    "    \"TGAC\",\n",
    "    \"TGAG\",\n",
    "    \"TGAT\",\n",
    "    \"TGCA\",\n",
    "    \"TGCC\",\n",
    "    \"TGCG\",\n",
    "    \"TGCT\",\n",
    "    \"TGGA\",\n",
    "    \"TGGC\",\n",
    "    \"TGGG\",\n",
    "    \"TGGT\",\n",
    "    \"TGTA\",\n",
    "    \"TGTC\",\n",
    "    \"TGTG\",\n",
    "    \"TGTT\",\n",
    "    \"TTAA\",\n",
    "    \"TTAC\",\n",
    "    \"TTAG\",\n",
    "    \"TTAT\",\n",
    "    \"TTCA\",\n",
    "    \"TTCC\",\n",
    "    \"TTCG\",\n",
    "    \"TTCT\",\n",
    "    \"TTGA\",\n",
    "    \"TTGC\",\n",
    "    \"TTGG\",\n",
    "    \"TTGT\",\n",
    "    \"TTTA\",\n",
    "    \"TTTC\",\n",
    "    \"TTTG\",\n",
    "    \"TTTT\",\n",
    "]\n",
    "\n",
    "\n",
    "def dump(value=None, filename=None):\n",
    "    if (value is not None) and (filename is not None):\n",
    "        joblib.dump(value=value, filename=filename)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Value & Filename should be passed.\".capitalize())\n",
    "\n",
    "\n",
    "def load(filename=None):\n",
    "    if filename is not None:\n",
    "        return joblib.load(filename=filename)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Filename should be passed in an appropriate manner\".capitalize()\n",
    "        )\n",
    "\n",
    "\n",
    "def config():\n",
    "    with open(\"../config.yml\", \"r\") as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "\n",
    "def hyperparameter_tuning(model: str = \"RF\"):\n",
    "    if model == \"RF\":\n",
    "        return {\n",
    "            \"n_estimators\": [100, 200, 300],\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        }\n",
    "    elif model == \"DT\":\n",
    "        return {\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"max_depth\": [None, 10, 20, 30],\n",
    "            \"min_samples_split\": [2, 5],\n",
    "            \"min_samples_leaf\": [1, 2],\n",
    "        }\n",
    "    elif model == \"LR\":\n",
    "        return {\n",
    "            \"penalty\": [\"l1\", \"l2\", \"elasticnet\", \"none\"],\n",
    "            \"C\": [0.001, 0.01, 0.1, 1, 10],\n",
    "            \"max_iter\": [100, 200, 300],\n",
    "        }\n",
    "    elif model == \"XGB\":\n",
    "        return {\n",
    "            \"learning_rate\": [0.01, 0.1, 1],\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "            \"n_estimators\": [100, 200, 300],\n",
    "        }\n",
    "    elif model == \"NB\":\n",
    "        return {\n",
    "            \"var_smoothing\": [1e-09],\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"The model name is not supported. Please check the model name and try again\".capitalize()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generator for DNA-Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:09<00:00,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset stored in the ../data/processed/ folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class FeatureGenerator:\n",
    "    def __init__(\n",
    "        self, approaches: list = [\"single\", \"di\", \"tri\", \"tetra\", \"gc-content\"]\n",
    "    ):\n",
    "        self.approaches = approaches\n",
    "\n",
    "        self.X = list()\n",
    "        self.y = list()\n",
    "\n",
    "        self.GC_Content = list()\n",
    "\n",
    "        self.dataset = pd.read_csv(\"../data/raw/DNA-Classification.csv\")[0:5] # I am using sub sample as it would take a huge time to craete\n",
    "\n",
    "    def feature_generator(self):\n",
    "        if \"single\" in self.approaches:\n",
    "            max_len = max(self.dataset[\"sequence\"].apply(len))\n",
    "\n",
    "            for pos in range(max_len):\n",
    "                for nucleoside in single_nucleosides:\n",
    "                    feature_column = f\"{nucleoside}_pos_{pos}\"\n",
    "                    self.dataset[feature_column] = 0\n",
    "\n",
    "            for instance in tqdm(range(self.dataset.shape[0])):\n",
    "                sequence = self.dataset.loc[instance, \"sequence\"]\n",
    "\n",
    "                for pos, nucleotide in enumerate(sequence):\n",
    "                    for nucleoside in single_nucleosides:\n",
    "                        feature_column = f\"{nucleoside}_pos_{pos}\"\n",
    "                        if nucleoside == nucleotide:\n",
    "                            self.dataset.loc[instance, feature_column] = 1\n",
    "\n",
    "        if \"di\" in self.approaches:\n",
    "            max_len = max(self.dataset[\"sequence\"].apply(len))\n",
    "\n",
    "            for pos in range(max_len - 1):\n",
    "                for di_nucleoside in di_nucleosides:\n",
    "                    feature_column = f\"{di_nucleoside}_pos_{pos}_di_nucleoside\"\n",
    "                    self.dataset[feature_column] = 0\n",
    "\n",
    "            for instance in tqdm(range(self.dataset.shape[0])):\n",
    "                sequence = self.dataset.loc[instance, \"sequence\"]\n",
    "                for pos in range(len(sequence) - 1):\n",
    "                    for di_nucleoside in di_nucleosides:\n",
    "                        feature_column = f\"{di_nucleoside}_pos_{pos}_di_nucleoside\"\n",
    "                        if sequence[pos : pos + 2] == di_nucleoside:\n",
    "                            self.dataset.loc[instance, feature_column] = 1\n",
    "\n",
    "        if \"tri\" in self.approaches:\n",
    "            max_len = max(self.dataset[\"sequence\"].apply(len))\n",
    "\n",
    "            for pos in range(max_len - 2):\n",
    "                for tri_nucleoside in tri_nucleosides:\n",
    "                    feature_column = f\"{tri_nucleoside}_pos_{pos}_tri_nucleoside\"\n",
    "                    self.dataset[feature_column] = 0\n",
    "\n",
    "            for instance in tqdm(range(self.dataset.shape[0])):\n",
    "                sequence = self.dataset.loc[instance, \"sequence\"]\n",
    "                for pos in range(len(sequence) - 2):\n",
    "                    for tri_nucleoside in tri_nucleosides:\n",
    "                        feature_column = f\"{tri_nucleoside}_pos_{pos}_tri_nucleoside\"\n",
    "                        if sequence[pos : pos + 3] == tri_nucleoside:\n",
    "                            self.dataset.loc[instance, feature_column] = 1\n",
    "\n",
    "        if \"tetra\" in self.approaches:\n",
    "            max_len = max(self.dataset[\"sequence\"].apply(len))\n",
    "\n",
    "            for pos in range(max_len - 3):\n",
    "                for tetra_nucleoside in tetra_nucleosides:\n",
    "                    feature_column = f\"{tetra_nucleoside}_pos_{pos}_tetra_nucleoside\"\n",
    "                    self.dataset[feature_column] = 0\n",
    "\n",
    "            for instance in tqdm(range(self.dataset.shape[0])):\n",
    "                sequence = self.dataset.loc[instance, \"sequence\"]\n",
    "                for pos in range(len(sequence) - 3):\n",
    "                    for tetra_nucleoside in tetra_nucleosides:\n",
    "                        feature_column = (\n",
    "                            f\"{tetra_nucleoside}_pos_{pos}_tetra_nucleoside\"\n",
    "                        )\n",
    "                        if sequence[pos : pos + 4] == tetra_nucleoside:\n",
    "                            self.dataset.loc[instance, feature_column] = 1\n",
    "\n",
    "        if \"gc-content\" in self.approaches:\n",
    "            self.GC_Content = []\n",
    "\n",
    "            for instance in tqdm(range(self.dataset.shape[0])):\n",
    "                sequence = self.dataset.loc[instance, \"sequence\"]\n",
    "                G_count = sequence.count(\"G\")\n",
    "                C_count = sequence.count(\"C\")\n",
    "                GC_Content = (\n",
    "                    (G_count + C_count) / len(sequence) if len(sequence) > 0 else 0\n",
    "                )\n",
    "                self.GC_Content.append(GC_Content)\n",
    "\n",
    "            self.dataset[\"GC-Content\"] = self.GC_Content\n",
    "\n",
    "        try:\n",
    "            self.dataset.to_csv(\n",
    "                os.path.join(\n",
    "                    config()[\"path\"][\"processed_path\"], \"processed_dataset.csv\"\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                \"Cannot saved the dataset in the processed file, & error: {}\".capitalize().format(\n",
    "                    e\n",
    "                )\n",
    "            )\n",
    "            traceback.print_exc()\n",
    "        else:\n",
    "            print(\n",
    "                \"the dataset stored in the {} folder\".format(\n",
    "                    config()[\"path\"][\"processed_path\"]\n",
    "                ).capitalize()\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generator = FeatureGenerator(approaches=[\"single\"])\n",
    "    generator.feature_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:09<00:00,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset stored in the ../data/processed/ folder\n",
      "Feature generation completed successfully and store in the folder ../data/processed/\n",
      "Dataset history is stored in the folder ../artifacts/files/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class DataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset=None,\n",
    "        split_size: float = 0.20,\n",
    "        approaches: list = [\"single\", \"di\", \"tri\", \"tetra\", \"gc_content\"],\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.split_size = split_size\n",
    "        self.approaches = approaches\n",
    "\n",
    "    def split_dataset(self):\n",
    "        if os.path.exists(config()[\"path\"][\"processed_path\"]):\n",
    "            dataset = os.path.join(\n",
    "                config()[\"path\"][\"processed_path\"], \"processed_dataset.csv\"\n",
    "            )\n",
    "\n",
    "            self.processed_data = pd.read_csv(dataset)\n",
    "\n",
    "            X = self.processed_data.iloc[:, 4:]\n",
    "            y = self.processed_data.iloc[:, 3]\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=self.split_size, random_state=42\n",
    "            )\n",
    "\n",
    "            for type, dataset in [\n",
    "                (\"X_train\", X_train),\n",
    "                (\"X_test\", X_test),\n",
    "                (\"y_train\", y_train),\n",
    "                (\"y_test\", y_test),\n",
    "            ]:\n",
    "                dataset.to_csv(\n",
    "                    os.path.join(config()[\"path\"][\"processed_path\"], f\"{type}.csv\"),\n",
    "                    index=False,\n",
    "                )\n",
    "\n",
    "            print(\n",
    "                \"Training and testing dataset is stored in the folder {}\".format(\n",
    "                    config()[\"path\"][\"processed_path\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"X_train\": X_train,\n",
    "                \"X_test\": X_test,\n",
    "                \"y_train\": y_train,\n",
    "                \"y_test\": y_test,\n",
    "            }\n",
    "\n",
    "    def feature_generator(self):\n",
    "        if isinstance(self.approaches, list):\n",
    "            self.generator = FeatureGenerator(approaches=self.approaches)\n",
    "\n",
    "            try:\n",
    "                self.generator.feature_generator()\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\".capitalize())\n",
    "            else:\n",
    "                print(\n",
    "                    \"Feature generation completed successfully and store in the folder {}\".format(\n",
    "                        os.path.join(config()[\"path\"][\"processed_path\"])\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError(\"Approaches must be a list\".capitalize())\n",
    "\n",
    "    @staticmethod\n",
    "    def dataset_history():\n",
    "        if os.path.exists(config()[\"path\"][\"processed_path\"]):\n",
    "            processed_path = os.path.join(\n",
    "                config()[\"path\"][\"processed_path\"], \"processed_dataset.csv\"\n",
    "            )\n",
    "\n",
    "            dataset = pd.read_csv(processed_path)\n",
    "\n",
    "            information = {}\n",
    "\n",
    "            information[\"isNaN\".title()] = (\n",
    "                \"NaN\".capitalize()\n",
    "                if dataset.isnull().sum().sum() > 0\n",
    "                else \"no NaN\".capitalize()\n",
    "            )\n",
    "\n",
    "            information[\"total_features\".title()] = str(dataset.shape[1])\n",
    "            information[\"total_instances\".title()] = str(dataset.shape[0])\n",
    "            information[\"dataset_shape\".title()] = str(dataset.shape)\n",
    "            information[\"target_ratio\".title()] = str(\n",
    "                dataset[\"labels\"].value_counts(ascending=False).to_dict()\n",
    "            )\n",
    "\n",
    "            pd.DataFrame(information, index=[0]).to_csv(\n",
    "                os.path.join(config()[\"path\"][\"files_path\"], \"dataset_history.csv\")\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                \"Dataset history is stored in the folder {}\".format(\n",
    "                    config()[\"path\"][\"files_path\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = DataLoader(\n",
    "        dataset=\"../data/raw/DNA-Classification.csv\",\n",
    "        split_size=0.2,\n",
    "        approaches=[\"single\"]  # please use \"di\", \"tri\", \"tetra\" and GC-content so create a huge amount of features\n",
    "    )\n",
    "    \n",
    "    loader.feature_generator()\n",
    "    \n",
    "    DataLoader.dataset_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
